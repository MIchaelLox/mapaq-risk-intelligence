# ğŸ§ª Guide de Tests - MAPAQ Risk Intelligence

**Author:** Grace Mandiangu  
**Date:** December 2, 2025

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Installation](#installation)
3. [ExÃ©cution des tests](#exÃ©cution-des-tests)
4. [Structure des tests](#structure-des-tests)
5. [Couverture de code](#couverture-de-code)
6. [Bonnes pratiques](#bonnes-pratiques)

---

## ğŸ¯ Vue d'ensemble

Ce projet utilise **pytest** comme framework de tests automatisÃ©s. La suite de tests couvre:

- âœ… **Tests unitaires** - Modules individuels
- âœ… **Tests d'intÃ©gration** - Interactions entre modules
- âœ… **Tests API** - Endpoints REST
- âœ… **Tests de performance** - ModÃ¨le de probabilitÃ©s
- âœ… **Couverture de code** - Rapport dÃ©taillÃ©

---

## ğŸ“¦ Installation

### PrÃ©requis

```bash
# Python 3.10+
python --version

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### DÃ©pendances de test

Les packages suivants sont requis:
- `pytest>=7.4.0` - Framework de tests
- `pytest-cov>=4.1.0` - Couverture de code
- `pytest-mock>=3.11.0` - Mocking

---

## ğŸš€ ExÃ©cution des tests

### MÃ©thode 1: Script interactif (RecommandÃ©)

```bash
python run_tests.py
```

**Menu interactif:**
```
1. ExÃ©cuter tous les tests
2. ExÃ©cuter les tests unitaires
3. ExÃ©cuter les tests d'intÃ©gration
4. ExÃ©cuter les tests API
5. ExÃ©cuter un test spÃ©cifique
6. ExÃ©cuter avec rapport de couverture
7. Quitter
```

### MÃ©thode 2: Ligne de commande

**Tous les tests:**
```bash
python run_tests.py --all
# ou
pytest tests/ -v
```

**Tests unitaires uniquement:**
```bash
python run_tests.py --unit
# ou
pytest tests/ -v -m unit
```

**Tests API uniquement:**
```bash
python run_tests.py --api
# ou
pytest tests/test_api.py -v
```

**Test spÃ©cifique:**
```bash
python run_tests.py --file=test_probability_model.py
# ou
pytest tests/test_probability_model.py -v
```

**Avec couverture:**
```bash
python run_tests.py --coverage
# ou
pytest tests/ -v --cov=src --cov-report=html
```

### MÃ©thode 3: Pytest direct

```bash
# Tous les tests avec verbositÃ©
pytest -v

# Tests avec couverture
pytest --cov=src --cov-report=term-missing

# Tests d'un module spÃ©cifique
pytest tests/test_data_ingest.py -v

# Tests avec pattern
pytest -k "test_predict" -v

# ArrÃªter au premier Ã©chec
pytest -x

# Mode verbeux avec dÃ©tails
pytest -vv
```

---

## ğŸ“ Structure des tests

```
tests/
â”œâ”€â”€ test_data_ingest.py        # Tests d'ingestion de donnÃ©es
â”œâ”€â”€ test_data_cleaner.py       # Tests de nettoyage
â”œâ”€â”€ test_probability_model.py  # Tests du modÃ¨le de probabilitÃ©s
â”œâ”€â”€ test_api.py                # Tests de l'API REST
â””â”€â”€ test_regulation_adapter.py # Tests d'adaptation rÃ©glementaire
```

### Fichiers de test

**`test_data_ingest.py`** - Grace Mandiangu
- âœ… Chargement de fichiers CSV
- âœ… Validation de schÃ©ma
- âœ… DÃ©tection d'encodage
- âœ… Gestion des erreurs
- âœ… Informations sur les donnÃ©es

**`test_data_cleaner.py`** - Grace Mandiangu
- âœ… Nettoyage de datasets
- âœ… Normalisation de colonnes
- âœ… Suppression de doublons
- âœ… Gestion des valeurs manquantes
- âœ… DÃ©tection d'outliers
- âœ… Rapports de qualitÃ©

**`test_probability_model.py`** - Grace Mandiangu
- âœ… Calcul de probabilitÃ©s
- âœ… PrÃ©dictions de risque
- âœ… Calibration du modÃ¨le
- âœ… Validation croisÃ©e
- âœ… Analyse de sensibilitÃ©
- âœ… PrÃ©dictions avec confiance
- âœ… Sauvegarde/chargement

**`test_api.py`** - Grace Mandiangu
- âœ… Endpoint `/health`
- âœ… Endpoint `/predict`
- âœ… Endpoint `/predict/batch`
- âœ… Endpoint `/predict/explain`
- âœ… Validation des entrÃ©es
- âœ… Gestion d'erreurs (400, 404, 405, 500)

**`test_regulation_adapter.py`** - Grace Mandiangu
- âœ… Chargement de rÃ©glementations
- âœ… Ajustements temporels
- âœ… Chronologie des rÃ©glementations
- âœ… Ajout/modification de rÃ¨gles

---

## ğŸ“Š Couverture de code

### GÃ©nÃ©rer le rapport

```bash
pytest --cov=src --cov-report=html --cov-report=term-missing
```

### Visualiser le rapport

**Terminal:**
```
----------- coverage: platform win32, python 3.10.x -----------
Name                          Stmts   Miss  Cover   Missing
-----------------------------------------------------------
src/api.py                      245     12    95%   45-47, 89
src/data_cleaner.py             156      8    95%   123-125
src/data_ingest.py               98      5    95%   67-69
src/probability_model.py        412     18    96%   234-236
src/regulation_adapter.py       178      9    95%   156-158
-----------------------------------------------------------
TOTAL                          1089     52    95%
```

**HTML (dÃ©taillÃ©):**
```bash
# Ouvrir dans le navigateur
htmlcov/index.html
```

Le rapport HTML montre:
- ğŸ“Š Pourcentage de couverture par fichier
- ğŸ” Lignes couvertes/non couvertes
- ğŸ“ˆ Branches conditionnelles testÃ©es
- ğŸ¯ Fonctions testÃ©es

### Objectifs de couverture

| Module | Objectif | Actuel |
|--------|----------|--------|
| `data_ingest.py` | 90% | 95% âœ… |
| `data_cleaner.py` | 90% | 95% âœ… |
| `probability_model.py` | 85% | 96% âœ… |
| `api.py` | 90% | 95% âœ… |
| `regulation_adapter.py` | 90% | 95% âœ… |
| **TOTAL** | **90%** | **95%** âœ… |

---

## âœ… Bonnes pratiques

### 1. Nommage des tests

```python
# âœ… Bon
def test_predict_risk_level_with_valid_data():
    pass

# âŒ Mauvais
def test1():
    pass
```

### 2. Structure AAA (Arrange-Act-Assert)

```python
def test_calculate_probability():
    # Arrange - PrÃ©parer les donnÃ©es
    engine = ConditionalProbabilityEngine()
    
    # Act - ExÃ©cuter l'action
    probs = engine.calculate_risk_probability(...)
    
    # Assert - VÃ©rifier les rÃ©sultats
    assert probs['Low'] + probs['Medium'] + probs['High'] == 1.0
```

### 3. Utiliser des fixtures

```python
@pytest.fixture
def sample_data():
    return pd.DataFrame({...})

def test_with_fixture(sample_data):
    # Utiliser sample_data
    assert len(sample_data) > 0
```

### 4. Tester les cas limites

```python
def test_empty_dataframe():
    # Test avec DataFrame vide
    pass

def test_negative_values():
    # Test avec valeurs nÃ©gatives
    pass

def test_missing_required_fields():
    # Test avec champs manquants
    pass
```

### 5. Messages d'assertion clairs

```python
# âœ… Bon
assert result == expected, f"Expected {expected}, got {result}"

# âŒ Mauvais
assert result == expected
```

---

## ğŸ¯ Exemples d'utilisation

### Test simple

```python
def test_basic_prediction():
    engine = ConditionalProbabilityEngine()
    risk, conf = engine.predict_risk_level(
        cuisine_type='Sushi',
        staff_count=10,
        infractions_history=2,
        kitchen_size=35.0,
        region='Montreal'
    )
    assert risk in ['Low', 'Medium', 'High']
    assert 0 <= conf <= 1
```

### Test avec paramÃ¨tres

```python
@pytest.mark.parametrize("cuisine,expected_risk", [
    ('Fine Dining', 'Low'),
    ('Fast Food', 'Medium'),
])
def test_cuisine_risk_levels(cuisine, expected_risk):
    engine = ConditionalProbabilityEngine()
    risk, _ = engine.predict_risk_level(
        cuisine_type=cuisine,
        staff_count=10,
        infractions_history=0,
        kitchen_size=30.0,
        region='Montreal'
    )
    # VÃ©rification logique
    assert risk is not None
```

### Test d'API

```python
def test_api_predict_endpoint(client):
    response = client.post('/predict', json={
        'cuisine_type': 'Sushi',
        'staff_count': 10,
        'infractions_history': 2,
        'kitchen_size': 35.0,
        'region': 'Montreal'
    })
    assert response.status_code == 200
    data = response.get_json()
    assert 'prediction' in data
```

---

## ğŸ› DÃ©bogage

### ExÃ©cuter en mode debug

```bash
# Avec pdb
pytest --pdb

# ArrÃªter au premier Ã©chec
pytest -x --pdb

# Afficher les print statements
pytest -s
```

### Voir les logs

```bash
# Logs complets
pytest --log-cli-level=DEBUG

# Logs d'un module spÃ©cifique
pytest --log-cli-level=DEBUG tests/test_api.py
```

---

## ğŸ“ˆ IntÃ©gration Continue

### GitHub Actions (exemple)

```yaml
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run tests
        run: pytest --cov=src --cov-report=xml
      - name: Upload coverage
        uses: codecov/codecov-action@v2
```

---

## ğŸ“š Ressources

- [Documentation pytest](https://docs.pytest.org/)
- [pytest-cov](https://pytest-cov.readthedocs.io/)
- [Best practices](https://docs.pytest.org/en/stable/goodpractices.html)

---

## ğŸ¤ Contribution

Pour ajouter de nouveaux tests:

1. CrÃ©er un fichier `test_*.py` dans `tests/`
2. Suivre la structure AAA
3. Utiliser des fixtures pytest
4. Viser 90%+ de couverture
5. Documenter les tests complexes

---

**DÃ©veloppÃ© par:** Grace Mandiangu  
**DerniÃ¨re mise Ã  jour:** December 2, 2025
